{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can NLP helps with musical genre classification?\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "As seen in the [previous project](https://github.com/gustavolopeso/spotify-genre-classifier), audio features extracted from songs by Spotify audio analysis software can be useful to help us to differentiate brazilian Rap from brazilian Indie. But how could we improve the classifier accuracy?\n",
    "\n",
    "Natural Language Processing (NLP) is a set of concepts and methods that look to make it possible for computers to understand natural human language. As rap and indie are different genres that are different in how they \"sound\", they are also different in what they talk about and how they do it.\n",
    "\n",
    "Brazilian rap is a genre well known for dealing with social problems, representing the urban peripheral youth. Its lyrics protest, tell real stories, and seek to bring a motivational message to those who listen.\n",
    "\n",
    "On the other hand, Brazilian Indie, which aggregates, in the case of this project, other genres such as New MPB and Alternative Rock, brings lyrics that deal with emotions and, to a certain extent, criticize the status quo. Many times the lyrics are not so obvious about the message they want to convey, being full of figures of speech, unlike rap, which is usually more direct.\n",
    "\n",
    "\n",
    "## 2. ETL\n",
    "\n",
    "We will use the song data extracted in the previous project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spotify_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to get the lyrics for each song in the dataframe. For that, we will use the [Letras.mus.br website](www.letras.mus.br). The url for accessing the lyrics page from a song has the following format:\n",
    "\n",
    "https://www.letras.mus.br/ARTIST_NAME/SONG_NAME\n",
    "\n",
    "A GET request will be made for each url and BeautifulSoup will help us to find the lyrics on the website html code. The lyrics will be stored in the dataframe as a list of string for each song.\n",
    "\n",
    "The get_lyrics function will be applied for the dataset to create the **lyrics** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(x):\n",
    "    try:\n",
    "        r = requests.get('https://www.letras.mus.br/{}/{}'.format(x['artist'].replace(' ','-').strip(),x['name'].replace(' ','-').strip()))\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        lyrics = list(soup.find_all(class_='cnt-letra')[0].find_all('p'))\n",
    "        lyrics = [str(item) for item in lyrics]\n",
    "        lyrics =  '\\n'.join(lyrics).replace('<br/>','\\n').replace('<p>','\\n').replace('</p>','\\n').split('\\n')\n",
    "        lyrics = [item for item in lyrics if item != '' and '[' not in item]\n",
    "        return lyrics\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1058/1058 [09:16<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "lyrics_list = []\n",
    "for i in tqdm(df.index):\n",
    "    row = df.iloc[i]\n",
    "    lyrics_list.append(get_lyrics(row))\n",
    "df['lyrics'] = lyrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lyrics for some songs couldn't be found, so we are going to drop these from the dataset. We found the lyrics for almost all songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>...</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>artist</th>\n",
       "      <th>name</th>\n",
       "      <th>album</th>\n",
       "      <th>release</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5zwvCa9LuVB46IQwKODSW3</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.696</td>\n",
       "      <td>5</td>\n",
       "      <td>-7.341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5zwvCa9LuVB4...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5zwv...</td>\n",
       "      <td>300587</td>\n",
       "      <td>4</td>\n",
       "      <td>Boogarins</td>\n",
       "      <td>Começa em Você</td>\n",
       "      <td>Manchaca, Vol. 2 (A Compilation of Boogarins M...</td>\n",
       "      <td>2021-04-02</td>\n",
       "      <td>indie</td>\n",
       "      <td>[Começa em você, Todo dedo sujo aponta, Encont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7gTAaRW4AqsArF1vDUFY03</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.819</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.520</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1520</td>\n",
       "      <td>0.0565</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7gTAaRW4AqsA...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7gTA...</td>\n",
       "      <td>281067</td>\n",
       "      <td>4</td>\n",
       "      <td>Boogarins</td>\n",
       "      <td>Correndo em Fúria</td>\n",
       "      <td>Manchaca, Vol. 2 (A Compilation of Boogarins M...</td>\n",
       "      <td>2021-04-02</td>\n",
       "      <td>indie</td>\n",
       "      <td>[Correndo em fúria, Contra toda essa angústia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4XLqe8UMsnlsaa2qguc5xW</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.828</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.863</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/4XLqe8UMsnls...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/4XLq...</td>\n",
       "      <td>129493</td>\n",
       "      <td>5</td>\n",
       "      <td>Boogarins</td>\n",
       "      <td>Vc Sabe Mto - Improviso Fábrica dos Sonhos</td>\n",
       "      <td>Manchaca, Vol. 2 (A Compilation of Boogarins M...</td>\n",
       "      <td>2021-04-02</td>\n",
       "      <td>indie</td>\n",
       "      <td>[Você sabe muito, Você sabe muito, Suas ideias...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3Tnaywzt8FACGnI0dBeeAv</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.769</td>\n",
       "      <td>6</td>\n",
       "      <td>-5.489</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>0.4470</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3Tnaywzt8FAC...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3Tna...</td>\n",
       "      <td>270653</td>\n",
       "      <td>4</td>\n",
       "      <td>Boogarins</td>\n",
       "      <td>Basic Lines</td>\n",
       "      <td>Manchaca, Vol. 2 (A Compilation of Boogarins M...</td>\n",
       "      <td>2021-04-02</td>\n",
       "      <td>indie</td>\n",
       "      <td>[Basic lines, Are like zig zag roads, Blowing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3blzViOH3HZuRnCfVIYuPg</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.824</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.838</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.4960</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3blzViOH3HZu...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3blz...</td>\n",
       "      <td>203453</td>\n",
       "      <td>4</td>\n",
       "      <td>Boogarins</td>\n",
       "      <td>No Meio de Tanto Cobertor - Medo de Falar</td>\n",
       "      <td>Manchaca, Vol. 2 (A Compilation of Boogarins M...</td>\n",
       "      <td>2021-04-02</td>\n",
       "      <td>indie</td>\n",
       "      <td>[Nem te achava, No meio de tanto cobertor, E a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1053</td>\n",
       "      <td>0qrsrne61cwFynJ0uzC0iS</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.705</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.525</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0qrsrne61cwF...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0qrs...</td>\n",
       "      <td>236058</td>\n",
       "      <td>4</td>\n",
       "      <td>Dalsin</td>\n",
       "      <td>Luxo</td>\n",
       "      <td>Cinza Chumbo</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>rap</td>\n",
       "      <td>[No janelão do apê mais alto da city, Vendo as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1054</td>\n",
       "      <td>0RKRkL6tM4OiBDaQ3qPUpL</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.526</td>\n",
       "      <td>9</td>\n",
       "      <td>-8.879</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0RKRkL6tM4Oi...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0RKR...</td>\n",
       "      <td>237567</td>\n",
       "      <td>4</td>\n",
       "      <td>Dalsin</td>\n",
       "      <td>Blindado</td>\n",
       "      <td>Cinza Chumbo</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>rap</td>\n",
       "      <td>[A meta é ser melhor que ontem meu chapa, Nem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1055</td>\n",
       "      <td>1Y64QrrZqNxbjnqsl9lxOl</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.226</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1Y64QrrZqNxb...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1Y64...</td>\n",
       "      <td>131591</td>\n",
       "      <td>4</td>\n",
       "      <td>Dalsin</td>\n",
       "      <td>Full</td>\n",
       "      <td>Cinza Chumbo</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>rap</td>\n",
       "      <td>[E eu faço essa porra virar antes que tu imagi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1056</td>\n",
       "      <td>2Y7sFVvhXLHrll5wm8eguy</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.578</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.127</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.3950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2Y7sFVvhXLHr...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2Y7s...</td>\n",
       "      <td>184941</td>\n",
       "      <td>5</td>\n",
       "      <td>Dalsin</td>\n",
       "      <td>Nave</td>\n",
       "      <td>Cinza Chumbo</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>rap</td>\n",
       "      <td>[Bateu neurose e ela correu pro mar, Com aquel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1057</td>\n",
       "      <td>15p4suAGKJyfCaocKZM7Im</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.417</td>\n",
       "      <td>8</td>\n",
       "      <td>-9.989</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>0.6640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/15p4suAGKJyf...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/15p4...</td>\n",
       "      <td>261165</td>\n",
       "      <td>4</td>\n",
       "      <td>Dalsin</td>\n",
       "      <td>Olhos Frios</td>\n",
       "      <td>Cinza Chumbo</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>rap</td>\n",
       "      <td>[É o gelo do sangue e o beijo da noiva mais fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1002 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                      id  danceability  energy  key  loudness  \\\n",
       "0         0  5zwvCa9LuVB46IQwKODSW3         0.509   0.696    5    -7.341   \n",
       "1         1  7gTAaRW4AqsArF1vDUFY03         0.491   0.819    9    -5.520   \n",
       "2         2  4XLqe8UMsnlsaa2qguc5xW         0.293   0.828    2    -6.863   \n",
       "3         3  3Tnaywzt8FACGnI0dBeeAv         0.724   0.769    6    -5.489   \n",
       "4         4  3blzViOH3HZuRnCfVIYuPg         0.578   0.824   11    -5.838   \n",
       "...     ...                     ...           ...     ...  ...       ...   \n",
       "997    1053  0qrsrne61cwFynJ0uzC0iS         0.695   0.705    1    -8.525   \n",
       "998    1054  0RKRkL6tM4OiBDaQ3qPUpL         0.391   0.526    9    -8.879   \n",
       "999    1055  1Y64QrrZqNxbjnqsl9lxOl         0.690   0.648    0    -8.226   \n",
       "1000   1056  2Y7sFVvhXLHrll5wm8eguy         0.592   0.578    3    -9.127   \n",
       "1001   1057  15p4suAGKJyfCaocKZM7Im         0.785   0.417    8    -9.989   \n",
       "\n",
       "      mode  speechiness  acousticness  instrumentalness  ...  \\\n",
       "0        1       0.0384        0.1100          0.434000  ...   \n",
       "1        1       0.1520        0.0565          0.000075  ...   \n",
       "2        1       0.0517        0.2590          0.001440  ...   \n",
       "3        0       0.0737        0.4470          0.015300  ...   \n",
       "4        1       0.0460        0.4960          0.003360  ...   \n",
       "...    ...          ...           ...               ...  ...   \n",
       "997      1       0.3100        0.1260          0.000000  ...   \n",
       "998      0       0.1800        0.3210          0.000000  ...   \n",
       "999      1       0.4870        0.2220          0.000000  ...   \n",
       "1000     0       0.3100        0.3950          0.000000  ...   \n",
       "1001     0       0.4550        0.6640          0.000000  ...   \n",
       "\n",
       "                                             track_href  \\\n",
       "0     https://api.spotify.com/v1/tracks/5zwvCa9LuVB4...   \n",
       "1     https://api.spotify.com/v1/tracks/7gTAaRW4AqsA...   \n",
       "2     https://api.spotify.com/v1/tracks/4XLqe8UMsnls...   \n",
       "3     https://api.spotify.com/v1/tracks/3Tnaywzt8FAC...   \n",
       "4     https://api.spotify.com/v1/tracks/3blzViOH3HZu...   \n",
       "...                                                 ...   \n",
       "997   https://api.spotify.com/v1/tracks/0qrsrne61cwF...   \n",
       "998   https://api.spotify.com/v1/tracks/0RKRkL6tM4Oi...   \n",
       "999   https://api.spotify.com/v1/tracks/1Y64QrrZqNxb...   \n",
       "1000  https://api.spotify.com/v1/tracks/2Y7sFVvhXLHr...   \n",
       "1001  https://api.spotify.com/v1/tracks/15p4suAGKJyf...   \n",
       "\n",
       "                                           analysis_url  duration_ms  \\\n",
       "0     https://api.spotify.com/v1/audio-analysis/5zwv...       300587   \n",
       "1     https://api.spotify.com/v1/audio-analysis/7gTA...       281067   \n",
       "2     https://api.spotify.com/v1/audio-analysis/4XLq...       129493   \n",
       "3     https://api.spotify.com/v1/audio-analysis/3Tna...       270653   \n",
       "4     https://api.spotify.com/v1/audio-analysis/3blz...       203453   \n",
       "...                                                 ...          ...   \n",
       "997   https://api.spotify.com/v1/audio-analysis/0qrs...       236058   \n",
       "998   https://api.spotify.com/v1/audio-analysis/0RKR...       237567   \n",
       "999   https://api.spotify.com/v1/audio-analysis/1Y64...       131591   \n",
       "1000  https://api.spotify.com/v1/audio-analysis/2Y7s...       184941   \n",
       "1001  https://api.spotify.com/v1/audio-analysis/15p4...       261165   \n",
       "\n",
       "     time_signature     artist                                        name  \\\n",
       "0                 4  Boogarins                              Começa em Você   \n",
       "1                 4  Boogarins                           Correndo em Fúria   \n",
       "2                 5  Boogarins  Vc Sabe Mto - Improviso Fábrica dos Sonhos   \n",
       "3                 4  Boogarins                                 Basic Lines   \n",
       "4                 4  Boogarins   No Meio de Tanto Cobertor - Medo de Falar   \n",
       "...             ...        ...                                         ...   \n",
       "997               4     Dalsin                                        Luxo   \n",
       "998               4     Dalsin                                    Blindado   \n",
       "999               4     Dalsin                                        Full   \n",
       "1000              5     Dalsin                                        Nave   \n",
       "1001              4     Dalsin                                 Olhos Frios   \n",
       "\n",
       "                                                  album     release  genre  \\\n",
       "0     Manchaca, Vol. 2 (A Compilation of Boogarins M...  2021-04-02  indie   \n",
       "1     Manchaca, Vol. 2 (A Compilation of Boogarins M...  2021-04-02  indie   \n",
       "2     Manchaca, Vol. 2 (A Compilation of Boogarins M...  2021-04-02  indie   \n",
       "3     Manchaca, Vol. 2 (A Compilation of Boogarins M...  2021-04-02  indie   \n",
       "4     Manchaca, Vol. 2 (A Compilation of Boogarins M...  2021-04-02  indie   \n",
       "...                                                 ...         ...    ...   \n",
       "997                                        Cinza Chumbo  2015-08-25    rap   \n",
       "998                                        Cinza Chumbo  2015-08-25    rap   \n",
       "999                                        Cinza Chumbo  2015-08-25    rap   \n",
       "1000                                       Cinza Chumbo  2015-08-25    rap   \n",
       "1001                                       Cinza Chumbo  2015-08-25    rap   \n",
       "\n",
       "                                                 lyrics  \n",
       "0     [Começa em você, Todo dedo sujo aponta, Encont...  \n",
       "1     [Correndo em fúria, Contra toda essa angústia,...  \n",
       "2     [Você sabe muito, Você sabe muito, Suas ideias...  \n",
       "3     [Basic lines, Are like zig zag roads, Blowing ...  \n",
       "4     [Nem te achava, No meio de tanto cobertor, E a...  \n",
       "...                                                 ...  \n",
       "997   [No janelão do apê mais alto da city, Vendo as...  \n",
       "998   [A meta é ser melhor que ontem meu chapa, Nem ...  \n",
       "999   [E eu faço essa porra virar antes que tu imagi...  \n",
       "1000  [Bateu neurose e ela correu pro mar, Com aquel...  \n",
       "1001  [É o gelo do sangue e o beijo da noiva mais fr...  \n",
       "\n",
       "[1002 rows x 25 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['lyrics'])\n",
    "df['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('spotify_lyrics_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2list(x): ### This function solves the problem of load dataframe .csv file containing list objects.\n",
    "    result_list = []\n",
    "    for i in x.split(\"',\"):\n",
    "        result_list.append(i.replace(\"'\",'').replace('[','').replace(']','').strip())\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spotify_lyrics_data.csv')\n",
    "df['lyrics'] = df['lyrics'].apply(str2list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Now that we have extracted the lyrics, we are going to use NLP to extract features from them. As said in the Overview section, the two genres are in different in **what** they talk about and **how** they do it. Therefore, we will use two methods to address these two problems.\n",
    "\n",
    "NLTK is one of the most important NLP toolkits for python, and we will use it in addition to sklearn nlp-related functions.\n",
    "\n",
    "### 3.1 Bag of Words\n",
    "\n",
    "Bag of Words is a very simple wayto extract features from text. It's based on counting the occurences of the words from a vocabulary in a text. For example, having the following vocabulary:\n",
    "\n",
    "- \"I\"\n",
    "- \"LOVE\"\n",
    "- \"SHE\"\n",
    "- \"APPLE\"\n",
    "- \"ME\"\n",
    "- \"HIM\"\n",
    "- \"MONEY\"\n",
    "- \"PEOPLE\"\n",
    "\n",
    "Considereing the following text:\n",
    "\n",
    "\"I LOVE MONEY. PEOPLE LOVE APPLES.\"\n",
    "\n",
    "We can now score this text according to that vocabulary:\n",
    "\n",
    "- \"I\": 1\n",
    "- \"LOVE\": 2\n",
    "- \"SHE\": 0\n",
    "- \"APPLES\": 1\n",
    "- \"ME\": 0\n",
    "- \"HIM\": 0\n",
    "- \"MONEY\": 1\n",
    "- \"PEOPLE\": 1\n",
    "\n",
    "The bag of words model doesn't capture any relationship between words or the order in which they are placed in the text, but instead focuses on the count of occurrences. It's easy to see that with Bag of Words we can turn complex texts into vectors of word occurences, what can be useful to train our classification model.\n",
    "\n",
    "In order to create the bag of words, we will need to transform the text into a list of \"tokens\", that is, groups of characters. These tokens will be stemmed, or, in another words, reduced to their stem, the \"root\" word that carries the meaning of the word. Finally, the stems will be counted and stored in a vector.\n",
    "\n",
    "#### 3.1.1 Word Tokenizing\n",
    "\n",
    "The tokenizing process could be done through some string splits, replaces and regex, but we will use nltk.word_tokenize() function to do it. For the bag of words model we are going to use the lyrics as an unique string, and not a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eu', 'sou', 'o', 'gustavo', '!', 'sou', 'brasileiro', '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lyrics_tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "lyrics_tokenize('Eu sou o Gustavo! Sou brasileiro!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Token Stemming\n",
    "\n",
    "Stemming is a good way for generalization. It makes it possible to understand the context more easily and to reduce the vocabulary complexity, reducing, in this way, the number of features, and, finally, the computational cost of training models.\n",
    "\n",
    "We are going to suppose that almost all lyrics are in portuguese. The language of the text is very important to the stemming process, because it's based on vocabulary dictionaries created for each language. Therefore, we will use the \"RSLP Stemmer\", a portuguese Stemmer created by **Viviane Moreira Orengo** and **Christian Huyck**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\irong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eu', 'sou', 'o', 'gustav', '!', 'sou', 'brasil', '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_stemming(tokens):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    stemmed_sentence = []\n",
    "    for token in tokens:\n",
    "        stemmed_sentence.append(stemmer.stem(token))\n",
    "    return stemmed_sentence\n",
    "token_stemming(lyrics_tokenize('Eu sou o Gustavo! Sou brasileiro!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Count Vectorizing\n",
    "\n",
    "Now we need to create our vocabulary and describe each lyrics as a vector of word counts from it. A good way to do that is counting the stem occurences for each lyrics, storing it in a dictionary. This dictionary will be appended to an auxiliary dataframe. If a new stem, that is, a stem that was not seen in any previous lyrics, is added, a new column will be created, and a NaN value will be assigned to all previous value in that column. Finally, the NaN values will be replaced by 0, indicating that this stem was not found in that lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1002/1002 [00:43<00:00, 22.87it/s]\n"
     ]
    }
   ],
   "source": [
    "bow_df = pd.DataFrame()\n",
    "dict_list = []\n",
    "for i in tqdm(df.reset_index().index):\n",
    "    row = df.iloc[i]\n",
    "    lyrics = ' '.join(row['lyrics'])\n",
    "    tokens = lyrics_tokenize(lyrics)\n",
    "    stemmed_tokens = token_stemming(tokens)\n",
    "    count_vector = {}\n",
    "    word_count = 0\n",
    "    for stemmed_token in stemmed_tokens:\n",
    "        word_count += 1\n",
    "        if stemmed_token in count_vector.keys():\n",
    "            count_vector[stemmed_token] += 1\n",
    "        else:\n",
    "            count_vector[stemmed_token] = 1\n",
    "    for key in count_vector.keys():\n",
    "        count_vector[key] = count_vector[key]/word_count\n",
    "    count_vector['word_count'] = word_count\n",
    "    count_vector['id'] = row['id']\n",
    "    count_vector['genre'] = row['genre']\n",
    "    dict_list.append(count_vector)\n",
    "bow_df = pd.DataFrame.from_records(dict_list)\n",
    "bow_df = bow_df.fillna(0)\n",
    "bow_df.to_csv('bow_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9165"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Analysis\n",
    "\n",
    "We got 9162 new features from the Bag of Words model. Maybe we could select only the most important ones to keep in our dataset, reducing the computational cost of dealing with a large number of features. For that, we are going to use Student's t to find out the tokens that are more likely to appear in one genre than in another. scipy module stats has a funcion called ttest_ind() that can help us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9166/9166 [08:41<00:00, 17.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "bow_p_dict = {}\n",
    "\n",
    "for col in tqdm(bow_df.columns):\n",
    "    if col not in ['id','genre','word_count']:\n",
    "        bow_p_dict[col] = ttest_ind(bow_df.loc[bow_df['genre'] == 'indie'][col],bow_df.loc[bow_df['genre'] == 'rap'][col])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to select the 50 features with the lowest calculated p-values to train our model. The smaller the p-value, the more different are the distributions of that stem in each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = bow_p_dict.keys()\n",
    "values = bow_p_dict.values()\n",
    "\n",
    "bow_p_df = pd.DataFrame(zip(bow_p_dict.keys(),bow_p_dict.values()))\n",
    "bow_p_df.columns = ['stem','p']\n",
    "bow_p_df = bow_p_df.sort_values(by='p')\n",
    "selected_words = list(bow_p_df.iloc[:50]['stem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the selected stems we can see that they are very common in rap songs and not in indie songs. We will use a strategy to keep the balance of the features.\n",
    "\n",
    "For each stem, we are going to check the genre it's more common to appear, so we can select characteristic stems from each genre. Maybe this approach can reduce the model accuracy, but if we want to increase the number of genres that the model is capable of classifying in the future, it's very important that it has information about all genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████████████                                                              | 1890/9162 [01:50<07:06, 17.03it/s]\n"
     ]
    }
   ],
   "source": [
    "genre_list = []\n",
    "genre_count = {}\n",
    "genres = bow_df['genre'].unique()\n",
    "for genre in genres:\n",
    "    genre_count[genre] = 0\n",
    "for stem in tqdm(bow_p_df['stem']):\n",
    "    max_mean = 0\n",
    "    for genre in genres:\n",
    "        if max_mean < bow_df.loc[bow_df['genre'] == genre][stem].mean():\n",
    "            max_genre = genre\n",
    "    genre_count[max_genre] += 1\n",
    "    flag = True\n",
    "    genre_list.append(max_genre)\n",
    "    for genre in genres:\n",
    "        if genre_count[genre] < 25:\n",
    "            flag = False\n",
    "    if flag:\n",
    "        break\n",
    "zeros = [0 for i in range(len(bow_p_df) - len(genre_list))]\n",
    "bow_p_df['genre'] = genre_list + zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = [None for i in range(len(bow_p_df) - len(genre_list))]\n",
    "bow_p_df['genre'] = genre_list + nans\n",
    "bow_p_df.columns = ['stem','p','genre']\n",
    "indie_stems = list(bow_p_df['stem'].loc[bow_p_df['genre'] == 'indie'][:25])\n",
    "rap_stems = list(bow_p_df['stem'].loc[bow_p_df['genre'] == 'rap'][:25])\n",
    "selected_stems = indie_stems + rap_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 Classification Perfomance\n",
    "\n",
    "Using the selected stems as features, the classification perfomance will be evaluated with the RandomForest Classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.9271\n",
      "Average ROC AUC score: 0.9760\n"
     ]
    }
   ],
   "source": [
    "bow_train_df = bow_df[['id']+selected_stems+['genre']].copy()\n",
    "bow_train_df['target'] = bow_train_df['genre'].map({'rap': True,'indie': False})\n",
    "X = bow_train_df[selected_stems]\n",
    "y = bow_train_df['target']\n",
    "acc = []\n",
    "auc = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    rfc = RandomForestClassifier(n_estimators = 200)\n",
    "    rfc.fit(X_train,y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test,y_pred))\n",
    "    y_score = rfc.predict_proba(X_test)[:, 1]\n",
    "    auc.append(roc_auc_score(y_test,y_score))\n",
    "print('Average accuracy: {:.4f}'.format(sum(acc)/len(acc)))\n",
    "print('Average ROC AUC score: {:.4f}'.format(sum(auc)/len(auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has achieved an average accuracy of 92.7%, which can be considered high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pos tagging\n",
    "\n",
    "The Part of Speech (POS) tagging process has the objetive of assigning a gramatical class for each word in a sentence. This method can, in a certain way, extract features about how the text message is delivered, that is, there are many ways to delivery the same message using text. Counting the frequencies of occurences of each POS in the lyrics is a good way to extract features about them. For example, considering the following sentence:\n",
    "\n",
    "\"I love to play soccer!\"\n",
    "\n",
    "We can POS tag the words as:\n",
    "\n",
    "- I: Personal Pronoun\n",
    "- love: Verb, 3rd person singular present\n",
    "- to: To (The word \"to\" is considered a POS tag)\n",
    "- play: Verb, base form\n",
    "- soccer: Noun, singular or mass\n",
    "\n",
    "A complete list of Parts of Speech is available [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "\n",
    "To do that, we will use a POS tagger created by [Matheus Inoue](https://github.com/inoueMashuu/POS-tagger-portuguese-nltk) for portuguese text. For this task it's important to have the lyrics separated line by line.\n",
    "\n",
    "We will need to tokenize the text again, so we will use the function lyrics_tokenize() created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "folder = 'trained_POS_taggers/'\n",
    "pos_tagger = joblib.load(folder+'POS_tagger_brill.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1002/1002 [00:36<00:00, 27.43it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_list = [] \n",
    "pos_df = pd.DataFrame()\n",
    "for i in tqdm(df.index):\n",
    "    row = df.iloc[i]\n",
    "    lyric = row['lyrics']\n",
    "    word_count = 0\n",
    "    pos_vector = {}\n",
    "    for phrase in lyric:\n",
    "        tokens = lyrics_tokenize(phrase)\n",
    "        if len(tokens) > 3:\n",
    "            for word in pos_tagger.tag(tokens):\n",
    "                if word[1] in pos_vector.keys():\n",
    "                    word_count += 1\n",
    "                    pos_vector[word[1]] += 1\n",
    "                else:\n",
    "                    word_count += 1\n",
    "                    pos_vector[word[1]] = 1\n",
    "    for key in pos_vector.keys():\n",
    "        pos_vector[key] = pos_vector[key]/word_count\n",
    "    pos_vector['id'] = row['id']\n",
    "    pos_vector['genre'] = row['genre']\n",
    "    pos_vector['word_count'] = word_count\n",
    "    dict_list.append(pos_vector)\n",
    "pos_df = pd.DataFrame.from_records(dict_list)\n",
    "pos_df = pos_df.fillna(0)\n",
    "pos_df.to_csv('pos_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Student's t test again to select the 25 most important feature to use in the training of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 353.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "pos_dict = {}\n",
    "\n",
    "for col in tqdm(pos_df.columns):\n",
    "    if col not in ['id','genre','word_count','name']:\n",
    "        pos_dict[col] = ttest_ind(pos_df.loc[pos_df['genre'] == 'indie'][col],pos_df.loc[pos_df['genre'] == 'rap'][col])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pos_dict.keys()\n",
    "values = pos_dict.values()\n",
    "\n",
    "pos_p_df = pd.DataFrame(zip(pos_dict.keys(),pos_dict.values()))\n",
    "pos_p_df.columns = ['pos','p']\n",
    "pos_p_df = pos_p_df.sort_values(by='p')\n",
    "selected_pos = list(pos_p_df.iloc[:25]['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the selected POS, we will repeat the classification evaluation process made to the selected stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.9143\n",
      "Average ROC AUC score: 0.9733\n"
     ]
    }
   ],
   "source": [
    "pos_train_df = pos_df[['id']+selected_pos+['genre']].copy()\n",
    "pos_train_df['target'] = pos_train_df['genre'].map({'rap': True,'indie': False})\n",
    "X = pos_train_df[selected_pos]\n",
    "y = pos_train_df['target']\n",
    "acc = []\n",
    "auc = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    rfc = RandomForestClassifier(n_estimators = 200)\n",
    "    rfc.fit(X_train,y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test,y_pred))\n",
    "    y_score = rfc.predict_proba(X_test)[:, 1]\n",
    "    auc.append(roc_auc_score(y_test,y_score))\n",
    "print('Average accuracy: {:.4f}'.format(sum(acc)/len(acc)))\n",
    "print('Average ROC AUC score: {:.4f}'.format(sum(auc)/len(auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has achieved an accuracy of 91.4%, which is lower than the accuracy obtained for the bag of words features and the audio features based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Model Training\n",
    "\n",
    "Firstly, we will create a dataset with all features (audio features, pos tags e stems) and store it in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = ['danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','time_signature']\n",
    "\n",
    "train_df = df[['id'] + audio_features + ['genre']].copy()\n",
    "\n",
    "bow_join_df = bow_train_df[['id']+selected_stems].copy()\n",
    "bow_join_df.columns = ['bow_'+x if x != 'id' else x for x in bow_join_df.columns]\n",
    "\n",
    "pos_join_df = pos_train_df[['id']+selected_pos].copy()\n",
    "pos_join_df.columns = ['pos_'+x if x != 'id' else x for x in pos_join_df.columns]\n",
    "\n",
    "train_df = train_df.merge(bow_join_df,on='id')\n",
    "train_df = train_df.merge(pos_join_df,on='id')\n",
    "train_df['target'] = train_df['genre'].map({'rap': True,'indie': False})\n",
    "train_df.to_csv('song_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_NPROP</th>\n",
       "      <th>pos_VAUX</th>\n",
       "      <th>pos_ADV-KS-REL</th>\n",
       "      <th>pos_PREP|+</th>\n",
       "      <th>pos_KC|[</th>\n",
       "      <th>pos_PCP</th>\n",
       "      <th>pos_PREP</th>\n",
       "      <th>pos_-</th>\n",
       "      <th>pos_?</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5zwvCa9LuVB46IQwKODSW3</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.696</td>\n",
       "      <td>5</td>\n",
       "      <td>-7.341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7gTAaRW4AqsArF1vDUFY03</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.819</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.520</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1520</td>\n",
       "      <td>0.0565</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4XLqe8UMsnlsaa2qguc5xW</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.828</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.863</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3Tnaywzt8FACGnI0dBeeAv</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.769</td>\n",
       "      <td>6</td>\n",
       "      <td>-5.489</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>0.4470</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.7260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3blzViOH3HZuRnCfVIYuPg</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.824</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.838</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.4960</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0qrsrne61cwFynJ0uzC0iS</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.705</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.525</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044655</td>\n",
       "      <td>0.014885</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>0.077131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0RKRkL6tM4OiBDaQ3qPUpL</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.526</td>\n",
       "      <td>9</td>\n",
       "      <td>-8.879</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049223</td>\n",
       "      <td>0.031088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.090674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1Y64QrrZqNxbjnqsl9lxOl</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.226</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074359</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020513</td>\n",
       "      <td>0.071795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2Y7sFVvhXLHrll5wm8eguy</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.578</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.127</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.3950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.074176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>15p4suAGKJyfCaocKZM7Im</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.417</td>\n",
       "      <td>8</td>\n",
       "      <td>-9.989</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>0.6640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016729</td>\n",
       "      <td>0.081784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1002 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  danceability  energy  key  loudness  mode  \\\n",
       "0     5zwvCa9LuVB46IQwKODSW3         0.509   0.696    5    -7.341     1   \n",
       "1     7gTAaRW4AqsArF1vDUFY03         0.491   0.819    9    -5.520     1   \n",
       "2     4XLqe8UMsnlsaa2qguc5xW         0.293   0.828    2    -6.863     1   \n",
       "3     3Tnaywzt8FACGnI0dBeeAv         0.724   0.769    6    -5.489     0   \n",
       "4     3blzViOH3HZuRnCfVIYuPg         0.578   0.824   11    -5.838     1   \n",
       "...                      ...           ...     ...  ...       ...   ...   \n",
       "997   0qrsrne61cwFynJ0uzC0iS         0.695   0.705    1    -8.525     1   \n",
       "998   0RKRkL6tM4OiBDaQ3qPUpL         0.391   0.526    9    -8.879     0   \n",
       "999   1Y64QrrZqNxbjnqsl9lxOl         0.690   0.648    0    -8.226     1   \n",
       "1000  2Y7sFVvhXLHrll5wm8eguy         0.592   0.578    3    -9.127     0   \n",
       "1001  15p4suAGKJyfCaocKZM7Im         0.785   0.417    8    -9.989     0   \n",
       "\n",
       "      speechiness  acousticness  instrumentalness  liveness  ...  pos_NPROP  \\\n",
       "0          0.0384        0.1100          0.434000    0.1250  ...   0.021277   \n",
       "1          0.1520        0.0565          0.000075    0.3520  ...   0.000000   \n",
       "2          0.0517        0.2590          0.001440    0.3960  ...   0.013699   \n",
       "3          0.0737        0.4470          0.015300    0.7260  ...   0.692308   \n",
       "4          0.0460        0.4960          0.003360    0.1650  ...   0.000000   \n",
       "...           ...           ...               ...       ...  ...        ...   \n",
       "997        0.3100        0.1260          0.000000    0.1200  ...   0.044655   \n",
       "998        0.1800        0.3210          0.000000    0.1400  ...   0.049223   \n",
       "999        0.4870        0.2220          0.000000    0.6570  ...   0.074359   \n",
       "1000       0.3100        0.3950          0.000000    0.0965  ...   0.065934   \n",
       "1001       0.4550        0.6640          0.000000    0.1360  ...   0.020446   \n",
       "\n",
       "      pos_VAUX  pos_ADV-KS-REL pos_PREP|+  pos_KC|[   pos_PCP  pos_PREP  \\\n",
       "0     0.000000        0.000000   0.000000  0.000000  0.021277  0.063830   \n",
       "1     0.000000        0.000000   0.000000  0.000000  0.000000  0.058824   \n",
       "2     0.000000        0.000000   0.013699  0.000000  0.000000  0.027397   \n",
       "3     0.000000        0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.000000        0.000000   0.000000  0.000000  0.000000  0.204545   \n",
       "...        ...             ...        ...       ...       ...       ...   \n",
       "997   0.014885        0.004060   0.001353  0.000000  0.018945  0.077131   \n",
       "998   0.031088        0.000000   0.000000  0.002591  0.020725  0.090674   \n",
       "999   0.007692        0.000000   0.000000  0.000000  0.020513  0.071795   \n",
       "1000  0.010989        0.002747   0.019231  0.000000  0.005495  0.074176   \n",
       "1001  0.018587        0.000000   0.003717  0.000000  0.016729  0.081784   \n",
       "\n",
       "      pos_-     pos_?  target  \n",
       "0       0.0  0.021277   False  \n",
       "1       0.0  0.000000   False  \n",
       "2       0.0  0.000000   False  \n",
       "3       0.0  0.000000   False  \n",
       "4       0.0  0.000000   False  \n",
       "...     ...       ...     ...  \n",
       "997     0.0  0.002706    True  \n",
       "998     0.0  0.000000    True  \n",
       "999     0.0  0.005128    True  \n",
       "1000    0.0  0.010989    True  \n",
       "1001    0.0  0.000000    True  \n",
       "\n",
       "[1002 rows x 90 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model Selection\n",
    "\n",
    "Now, we need to select which model we will use to the classifier. As we did in the previous project, we will test the following models:\n",
    "\n",
    "- K Nearest Neighbors\n",
    "- Support Vector Machine\n",
    "- Random Forest\n",
    "\n",
    "#### 4.1.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 1} 0.8422475247524751\n"
     ]
    }
   ],
   "source": [
    "features = [x for x in train_df.columns if x not in ['id','genre','target']]\n",
    "X = train_df[features]\n",
    "y = train_df['target']\n",
    "acc = []\n",
    "auc = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': [1,3,5,10,20,30]}\n",
    "clf = GridSearchCV(knn,param_grid,cv=10)\n",
    "clf.fit(X,y)\n",
    "params = clf.cv_results_['params']\n",
    "score = list(clf.cv_results_['mean_test_score'])\n",
    "index = score.index(max(score))\n",
    "print(params[index],score[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum accuracy (84.2%) was reached for KNN algorithm with 1 neighbor.\n",
    "\n",
    "#### 4.1.2 SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'kernel': 'rbf'} 0.8831089108910891\n"
     ]
    }
   ],
   "source": [
    "features = [x for x in train_df.columns if x not in ['id','genre','target']]\n",
    "X = train_df[features]\n",
    "y = train_df['target']\n",
    "acc = []\n",
    "auc = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "svc = SVC()\n",
    "param_grid = {'kernel': ['linear','rbf'], 'C': [0.01,0.1,1,10]}\n",
    "clf = GridSearchCV(svc,param_grid,cv=10)\n",
    "clf.fit(X,y)\n",
    "params = clf.cv_results_['params']\n",
    "score = list(clf.cv_results_['mean_test_score'])\n",
    "index = score.index(max(score))\n",
    "print(params[index],score[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC model with a C of 10 and a rbf kernel reached 88.3% of accuracy, which is considerably higher than the accuracy of the KNN model.\n",
    "\n",
    "#### 4.1.3 RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100} 0.9410792079207921\n"
     ]
    }
   ],
   "source": [
    "features = [x for x in train_df.columns if x not in ['id','genre','target']]\n",
    "X = train_df[features]\n",
    "y = train_df['target']\n",
    "acc = []\n",
    "auc = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "rfc = RandomForestClassifier()\n",
    "param_grid = {'n_estimators': [20,50,100,200,500]}\n",
    "clf = GridSearchCV(rfc,param_grid,cv=10)\n",
    "clf.fit(X,y)\n",
    "params = clf.cv_results_['params']\n",
    "score = list(clf.cv_results_['mean_test_score'])\n",
    "index = score.index(max(score))\n",
    "print(params[index],score[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFC outperformed both KNN and SVC, reaching 94.1% accuracy with a forest of 100 trees each trained on a random slice of the dataset, which is expected because of the results of the previous project.\n",
    "\n",
    "### 4.2 Performance Evaluation\n",
    "\n",
    "So, using the Random Forest Classifier with 100 estimators we will check the accuracy and the AUC-ROC metric from the classificator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[features]\n",
    "y = train_df['target']\n",
    "acc = []\n",
    "auc = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    rfc = RandomForestClassifier(n_estimators = 100)\n",
    "    rfc.fit(X_train,y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test,y_pred))\n",
    "    y_score = rfc.predict_proba(X_test)[:, 1]\n",
    "    auc.append(roc_auc_score(y_test,y_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.9653\n",
      "Average ROC AUC score: 0.9917\n"
     ]
    }
   ],
   "source": [
    "print('Average accuracy: {:.4f}'.format(sum(acc)/len(acc)))\n",
    "print('Average ROC AUC score: {:.4f}'.format(sum(auc)/len(auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got 96.5% accuracy and 0.99 of ROC AUC score. This result is significantly better than the result achieved by the RFC trained with audio features only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "\n",
    "As seen, the addition of BOW and POS features extracted by NLP increased the accuracy of the RFC model trained on the dataset by almost 5% (91.9% -> 96.5%). We usually associate NLP with sentiment analysis and other complex tasks such as topic classification and translation, but NLP includes other simpler concepts such as Bag of Words and Part of Speech tagging, which can be very useful for extracting features from text without requiring a lot of computational power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
